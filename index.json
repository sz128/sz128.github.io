[{"authors":["admin"],"categories":null,"content":"I am a PhD student at Shanghai Jiao Tong University, supervised by Kai Yu. My research interests include spoken/natural language understanding, dialogue systems, semantic parsing, text2SQL, and structured deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sz128.github.io/author/su-zhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/su-zhu/","section":"authors","summary":"I am a PhD student at Shanghai Jiao Tong University, supervised by Kai Yu. My research interests include spoken/natural language understanding, dialogue systems, semantic parsing, text2SQL, and structured deep learning.","tags":null,"title":"Su Zhu","type":"authors"},{"authors":null,"categories":[],"content":"","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591488000,"objectID":"8979940e8ca1f1cbe35482def083f5b9","permalink":"https://sz128.github.io/publication/2020-dual-learning-for-semi-supervised-natural-language-understanding/","publishdate":"2020-06-07T00:00:00Z","relpermalink":"/publication/2020-dual-learning-for-semi-supervised-natural-language-understanding/","section":"publication","summary":"Natural language understanding (NLU) converts sentences into structured semantic forms. The paucity of annotated training samples is still a fundamental challenge of NLU. To solve this data sparsity problem, previous work based on semi-supervised learning mainly focuses on exploiting unlabeled sentences. In this work, we introduce a dual task of NLU, semantic-to-sentence generation (SSG), and propose a new framework for semi-supervised NLU with the corresponding dual model. The framework is composed of dual pseudo-labeling and dual learning method, which enables an NLU model to make full use of data (labeled and unlabeled) through a closed-loop of the primal and dual tasks. By incorporating the dual task, the framework can exploit pure semantic forms as well as unlabeled sentences, and further improve the NLU and SSG models iteratively in the closed-loop. The proposed approaches are evaluated on two public datasets (ATIS and SNIPS). Experiments in the semi-supervised setting show that our methods can outperform various baselines significantly, and extensive ablation studies are conducted to verify the effectiveness of our framework. Finally, our method can also achieve the state-of-the-art performance on the two datasets in the supervised setting.","tags":[],"title":"Dual Learning for Semi Supervised Natural Language Understanding","type":"publication"},{"authors":null,"categories":["preprint"],"content":"","date":1590278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590278400,"objectID":"a98cd9f93baa94f72ea0d8fde6a39bce","permalink":"https://sz128.github.io/publication/2020-bert-wcn/","publishdate":"2020-05-24T00:00:00Z","relpermalink":"/publication/2020-bert-wcn/","section":"publication","summary":"","tags":[],"title":"Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding","type":"publication"},{"authors":null,"categories":[],"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"46b769e785b9069e8d6d5b88922db829","permalink":"https://sz128.github.io/publication/2020-acl-zhao_yanbin/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020-acl-zhao_yanbin/","section":"publication","summary":"","tags":[],"title":"Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks","type":"publication"},{"authors":null,"categories":[],"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"0be6be5aae05785434ac300ef2012ae5","permalink":"https://sz128.github.io/publication/2020-acl-chen_lu/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020-acl-chen_lu/","section":"publication","summary":"","tags":[],"title":"Neural Graph Matching Networks for Chinese Short Text Matching","type":"publication"},{"authors":null,"categories":[],"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"062b5769e52ac8126de09403b6c77047","permalink":"https://sz128.github.io/publication/2020-acl-semantic_parsing/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020-acl-semantic_parsing/","section":"publication","summary":"","tags":[],"title":"Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing","type":"publication"},{"authors":null,"categories":[],"content":"","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587254400,"objectID":"b6d4c997c142933b0f414aba474ec817","permalink":"https://sz128.github.io/publication/2020-icassp-hd_dst/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/publication/2020-icassp-hd_dst/","section":"publication","summary":"","tags":[],"title":"A Hierarchical Tracker For Multi-domain Dialogue State Tracking","type":"publication"},{"authors":null,"categories":["preprint"],"content":"","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586476800,"objectID":"8667c21c1f7e1199ece5125919d7d8de","permalink":"https://sz128.github.io/publication/2020-tranformer-dst/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/publication/2020-tranformer-dst/","section":"publication","summary":"","tags":[],"title":"Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking","type":"publication"},{"authors":null,"categories":[],"content":"","date":1584144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584144000,"objectID":"47f955a4c76584090424807a4741a57e","permalink":"https://sz128.github.io/publication/2020-taslp-label_embeddings/","publishdate":"2020-03-14T00:00:00Z","relpermalink":"/publication/2020-taslp-label_embeddings/","section":"publication","summary":"","tags":[],"title":"Prior Knowledge Driven Label Embedding for Slot Filling in Natural Language Understanding","type":"publication"},{"authors":null,"categories":[],"content":"","date":1583280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583280000,"objectID":"842d2012e80e40c858d05cc1aa02f590","permalink":"https://sz128.github.io/publication/2020-aaai-chen_lu/","publishdate":"2020-03-04T00:00:00Z","relpermalink":"/publication/2020-aaai-chen_lu/","section":"publication","summary":"","tags":[],"title":"Schema-Guided Multi-Domain Dialogue State Tracking with Graph Attention Neural Networks","type":"publication"},{"authors":null,"categories":[],"content":"","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572825600,"objectID":"084d452f366f73531758d83b17edfbc2","permalink":"https://sz128.github.io/publication/2019-emnlp-atomic_template/","publishdate":"2019-11-04T00:00:00Z","relpermalink":"/publication/2019-emnlp-atomic_template/","section":"publication","summary":"","tags":[],"title":"Data Augmentation with Atomic Templates for Spoken Language Understanding","type":"publication"},{"authors":null,"categories":[],"content":"","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"9f45ac036f5b7d1eb85729ad7996ae02","permalink":"https://sz128.github.io/publication/2019-icmi-catslu/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/publication/2019-icmi-catslu/","section":"publication","summary":"","tags":[],"title":"CATSLU: The 1st Chinese Audio-Textual Spoken Language Understanding Challenge","type":"publication"},{"authors":null,"categories":[],"content":"","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"9ab9fae4b3263d2842bd24d8c40d67b9","permalink":"https://sz128.github.io/publication/2019-icmi-robust_slu/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/publication/2019-icmi-robust_slu/","section":"publication","summary":"","tags":[],"title":"Robust Spoken Language Understanding with Acoustic and Domain Knowledge","type":"publication"},{"authors":null,"categories":[],"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"5d45e025f4fb0022f4537a37e48442b1","permalink":"https://sz128.github.io/publication/2019-acl-dual_semantic_parsing/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/publication/2019-acl-dual_semantic_parsing/","section":"publication","summary":"","tags":[],"title":"Semantic Parsing with Dual Learning","type":"publication"},{"authors":null,"categories":[],"content":"","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558051200,"objectID":"3bda7238180888f947bf77e96a804e39","permalink":"https://sz128.github.io/publication/2019-icassp-hdslu/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/publication/2019-icassp-hdslu/","section":"publication","summary":"","tags":[],"title":"A Hierarchical Decoding Model for Spoken Language Understanding from Unaligned Data","type":"publication"},{"authors":null,"categories":[],"content":"","date":1534636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534636800,"objectID":"34bd93152cd490575e2cda711824f76f","permalink":"https://sz128.github.io/publication/2018-iscide-joint_slu_lm/","publishdate":"2018-08-19T00:00:00Z","relpermalink":"/publication/2018-iscide-joint_slu_lm/","section":"publication","summary":"","tags":[],"title":"Joint Spoken Language Understanding and Domain Adaptive Language Modeling","type":"publication"},{"authors":null,"categories":[],"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"65dad328fc33d12038773589f2b07999","permalink":"https://sz128.github.io/publication/2018-sigdial-adaptivelu/","publishdate":"2018-08-01T00:00:00Z","relpermalink":"/publication/2018-sigdial-adaptivelu/","section":"publication","summary":"","tags":[],"title":"Concept Transfer Learning for Adaptive Language Understanding","type":"publication"},{"authors":null,"categories":[],"content":"","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"2765b76c4f395053a0d9eff4771c72dc","permalink":"https://sz128.github.io/publication/2018-icassp-asr_error_adaptation/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/publication/2018-icassp-asr_error_adaptation/","section":"publication","summary":"","tags":[],"title":"Robust Spoken Language Understanding with Unsupervised ASR-error Adaptation","type":"publication"},{"authors":null,"categories":[],"content":"","date":1524182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524182400,"objectID":"9bf21e3cc59d18e661fdec62213e6a6e","permalink":"https://sz128.github.io/publication/2018-icassp-semi_supervised_slu/","publishdate":"2018-04-20T00:00:00Z","relpermalink":"/publication/2018-icassp-semi_supervised_slu/","section":"publication","summary":"","tags":[],"title":"Semi-Supervised Training Using Adversarial Multi-Task Learning for Spoken Language Understanding","type":"publication"},{"authors":null,"categories":[],"content":"","date":1489017600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489017600,"objectID":"000d35032adfa6ba4b32f2a6597e3377","permalink":"https://sz128.github.io/publication/2017-icassp-focus/","publishdate":"2017-03-09T00:00:00Z","relpermalink":"/publication/2017-icassp-focus/","section":"publication","summary":"","tags":[],"title":"Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding","type":"publication"},{"authors":["Su Zhu"],"categories":["paper_review"],"content":"2017鸡年第一波论文阅读总结及推荐 引\n 本次推荐论文如下：\n Structured prediction models for RNN based sequence labeling in clinical text. EMNLP 2016. Sentence rewriting for semantic parsing. ACL 2016. Adversarial training methods for semi-supervised text classification. Under review as a conference paper at ICLR 2017.   Structured prediction models for RNN based sequence labeling in clinical text \n1 作者 Abhyuday N Jagannatha, Hong Yu\n单位 University of Massachusetts, MA, USA Bedford VAMC and CHOIR, MA, USA\n关键词 sequence labeling, skip chain CRF, Bi-LSTM\n来源 EMNLP 2016\n立题 解决医用领域的slot filling问题。\n模型 （1）Bi-LSTM CRF Bi-LSTM 的输出层套上CRF的criterion，可以参考:[Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991.]\n（2）Bi-LSTM CRF with pairwise modeling 因为很多label的连接在训练数据中出现的次数很少，无法被很好地学习到。于是作者把CRF中$$A_{y_t,y_{t+1}}$$的label transition score用一个简单的神经网络模型来预测，这个NN模型的输入是t时刻和t+1时刻的隐层状态。\n（3）Bi-LSTM with approximate Skip-chain CRF 简评 这是一个对Bi-LSTM CRF深入研究的工作。\nSentence rewriting for semantic parsing \n2 作者 Bo Chen, Le Sun, Xianpei Han, Bo An\n单位 State Key Laboratory of Computer Sciences Institute of Software, Chinese Academy of Sciences, China.\n关键词 Sentence rewriting, semantic parsing\n来源 ACL 2016\n立题 由于semantic parsing的目标logical form是依赖于ontology的词典的，所以有一些语义相同但是语言表达形式稀少的句子很难被解析正确。为了解决这个问题，该文章提出使用句子转写的方式，把语言表达形式稀少的句子转成意思相同且常见的句子。\n模型 （1）Dictionary-based Rewriting 基于字典的名词再解释。\n做法举例：“For instance, the word “daughter” is explained as “female child” in Wiktionary”\n使用到资源：Wiktionary\n（2）Template-based Rewriting 基于句子模板的替换。 使用到资源：WikiAnswers\n简评 句子重写是一种解决数据稀疏问题的逆向方法。\nAdversarial training methods for semi-supervised text classification \n3 作者 Takeru Miyato, Andrew M Dai, Ian Goodfellow\n单位 Kyoto University, Google Brain and OpenAI\n关键词 Adversarial training, virtual adversarial training, semi-supervised learning, text classification\n来源 ICLR 2017 （under review）\n立题 把GAN学习应用到NLP里来。\n模型 （1）Adversarial training 有监督学习。\n（2）Virtual adversarial training 无监督学习。\n简评 提供了一种regularization的方案。\n","date":1486684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486684800,"objectID":"82da4e6931981756cef7152f4ed41284","permalink":"https://sz128.github.io/post/paper-reading-2017-02-10/","publishdate":"2017-02-10T00:00:00Z","relpermalink":"/post/paper-reading-2017-02-10/","section":"post","summary":"2017鸡年第一波论文阅读总结及推荐 引\n 本次推荐论文如下：\n Structured prediction models for RNN based sequence labeling in clinical text. EMNLP 2016. Sentence rewriting for semantic parsing. ACL 2016. Adversarial training methods for semi-supervised text classification.","tags":[],"title":"Paper Reading 2017 02 10","type":"post"},{"authors":["Su Zhu"],"categories":["review"],"content":"Table of Contents  前言 论文调研  调研方向 调研方式 文章汇总     前言 在任务型的口语对话系统框架下，目前主流的口语语义理解方法主要分为基于规则、统计学习的方法。在建立一个口语语义解析器之初，领域专家或者开发者往往需要人为定义语义框架、语义槽以及符合对话场景的所有可能的槽值。基于规则的方法，则继续需要领域专家或者开发人员人为地编写、维护规则文法。基于统计学习的方法则需要数据标注人员标注大量的数据，用于有监督学习。所以现在主流的口语语义解析器的构建过程可以描述为如下几个步骤：\n  定义指定领域的语义表示方式 编写规则/(收集数据+人工标注) 生成模型 校验   然而其中有几大问题，使得任务型的口语义理解无法快速地构建或者领域迁移。\n第一个问题是“语义表示方式的定义”。目前绝大多数的方法都使用意图+槽值对的方式来表示一句话的语义。这样简单的设计对于单个领域或者应用是非常高效的，然而对于领域扩展、迁移却并不友好。比如天气领域中有语义槽“city”，机票领域中有语义槽“Fromcity”和“Tocity”，这三个语义槽在这种扁平化的设计里是属于同一层次的，然而这并不符合人类语言现象。是否改为“city”、“from.city”、“to.city”会好一点呢？那么有层次的语义槽设计具体应该是怎样的呢？\n第二个问题是“规则编写”和“数据标注”。数据收集和数据标注是非常耗费人力和财力的，而相比之下规则编写要轻松一些。然而目前人工编写规则有如下主要缺点：第一，人力消耗大；第二，难维护；第三，难移植到新领域。所以我们是否可以自动生成规则？是否可以提升规则系统的易维护性？是否可以实现规则的跨领域移植？\n那为什么我们选择基于规则检索的方法做口语义理解？原因之一是基于规则方法的可控性要高于数据驱动的基于统计学习的方法（加入一条新规则比加入一类新数据且重训模型要可控）；更重要的是语义规则是一种高层知识的体现，是很难直接从有限的数据中学习到。为此，我们希望从从基于规则的方法出发，力图解决上述几个问题，实现“开放式、可扩展”的口语语义理解。\n论文调研 调研方向  规则生成 规则检索 语义表示 语义项自动导出 规则与统计方法相结合  调研方式 已知的语义库  FrameNet/PropBank/AMR Freebase  google搜索 关键词\n grammar/rule/template retrieval [+semantic] semantic representation  会议期刊论文集 近三年相关论文，搜索关键词为:retrieval/rule/grammar/slot/semantic_representation\n Interspeech/ICASSP/ASRU/SLT/Sigdial ACL/EMNLP/COLING/NAACL TASLP AAAI/NIPS  文章汇总 规则生成  SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems. SemEval 2014. SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using CF-IRF Rule Similarity. SemEval 2014. tucSage: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection. SemEval 2014. Web Data Harvesting for Speech Understanding Grammar Induction. Interspeech 2013. Using Lexical, Syntactic And Semantic Features For Non-terminal Grammar Rule Induction In Spoken Dialogue Systems. SLT 2014. Fusion Of Knowledge-based And Data-driven Approaches To Grammar Induction. Interspeech 2014. Semi-automatic Acquisition Of Domain-specific Semantic Structures. EuroSpeech 1999. Semiautomatic acquisition of semantic structures for understanding domain-specific natural language queries. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2002.  规则检索  Information Retrieval Techniques in Rule-based Expert Systems. CDAKO 1991. A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation. ACL 2016. Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation. EMNLP 2015.  语义表示  Spoken language understanding. IEEE Signal Processing Magazine 2005. Spoken Language Understanding-Interpreting the signs given by a speech signal. IEEE Signal Processing Magazine 2008. A SURVEY ON ONTOLOGY CONSTRUCTION METHODOLOGIES. ECBS 2011. Semantic Representation. De-Conflated Semantic Representations. A Domain-independent Rule-based Framework for Event Extraction. Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity. Machine Comprehension using Rich Semantic Representations.  语义项自动导出  Unsupervised Induction And Filling Of Semantic Slots For Spoken Dialogue Systems Using Frame-semantic Parsing. ASRU 2013. Leveraging Frame Semantics And Distributional Semantics For Unsupervised Semantic Slot Induction In Spoken Dialogue Systems. SLT 2014. Dynamically Supporting Unexplored Domains In Conversational Interactions By Enriching Semantics With Neural Word Embeddings. SLT 2014. Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding. ACL 2015.  统计+规则  Harnessing Deep Neural Networks with Logic Rules. ACL 2016. Semantic Parsing using Distributional Semantics and Probabilistic Logic. The ACL 2014 Workshop on Semantic Parsing.  ","date":1477526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477526400,"objectID":"dc79cee8447d8a6c102d68db44e3b73e","permalink":"https://sz128.github.io/post/survey-on-grammar-retrieval/","publishdate":"2016-10-27T00:00:00Z","relpermalink":"/post/survey-on-grammar-retrieval/","section":"post","summary":"Table of Contents  前言 论文调研  调研方向 调研方式 文章汇总     前言 在任务型的口语对话系统框架下，目前主流的口语语义理解方法主要分为基于规则、统计学习的方法。在建立一个口语语义解析器之初，领域专家或者开发者往往需要人为定义语义框架、语义槽以及符合对话场景的所有可能的槽值。基于规则的方法，则继续需要领域专家或者开发人员人为地编写、维护规则文法。基于统计学习的方法则需要数据标注人员标注大量的数据，用于有监督学习。所以现在主流的口语语义解析器的构建过程可以描述为如下几个步骤：\n  定义指定领域的语义表示方式 编写规则/(收集数据+人工标注) 生成模型 校验   然而其中有几大问题，使得任务型的口语义理解无法快速地构建或者领域迁移。\n第一个问题是“语义表示方式的定义”。目前绝大多数的方法都使用意图+槽值对的方式来表示一句话的语义。这样简单的设计对于单个领域或者应用是非常高效的，然而对于领域扩展、迁移却并不友好。比如天气领域中有语义槽“city”，机票领域中有语义槽“Fromcity”和“Tocity”，这三个语义槽在这种扁平化的设计里是属于同一层次的，然而这并不符合人类语言现象。是否改为“city”、“from.city”、“to.city”会好一点呢？那么有层次的语义槽设计具体应该是怎样的呢？\n第二个问题是“规则编写”和“数据标注”。数据收集和数据标注是非常耗费人力和财力的，而相比之下规则编写要轻松一些。然而目前人工编写规则有如下主要缺点：第一，人力消耗大；第二，难维护；第三，难移植到新领域。所以我们是否可以自动生成规则？是否可以提升规则系统的易维护性？是否可以实现规则的跨领域移植？\n那为什么我们选择基于规则检索的方法做口语义理解？原因之一是基于规则方法的可控性要高于数据驱动的基于统计学习的方法（加入一条新规则比加入一类新数据且重训模型要可控）；更重要的是语义规则是一种高层知识的体现，是很难直接从有限的数据中学习到。为此，我们希望从从基于规则的方法出发，力图解决上述几个问题，实现“开放式、可扩展”的口语语义理解。\n论文调研 调研方向  规则生成 规则检索 语义表示 语义项自动导出 规则与统计方法相结合  调研方式 已知的语义库  FrameNet/PropBank/AMR Freebase  google搜索 关键词","tags":[],"title":"Survey on Grammar Retrieval","type":"post"},{"authors":null,"categories":[],"content":"","date":1476921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476921600,"objectID":"308d1ac40730c8573f006b474d84ea68","permalink":"https://sz128.github.io/publication/2016-iscslp-wu_xueyang/","publishdate":"2016-10-20T00:00:00Z","relpermalink":"/publication/2016-iscslp-wu_xueyang/","section":"publication","summary":"","tags":[],"title":"Rich Punctuations Prediction Using Large-scale Deep Learning","type":"publication"},{"authors":["Su Zhu"],"categories":["paper_review"],"content":"Language to Logic Form with Neural Attention 作者 Li Dong, Mirella Lapata li.dong@ed.ac.uk, mlap@inf.ed.ac.uk\n单位 Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB\n关键词 sequence-to-sequence, sequence-to-tree, semantic parsing\n来源 ACL 2016\n立题 该文涉及的任务是semantic parsing，其目标是将一句话解析为正式的意图表示（比如一个逻辑表达式或者结构化的query）。作者首次将seq2seq引入该任务，并在普通seq2seq的decoder无法考虑逻辑表达式的层次结构的问题上，创造性地提出了层次树decoder。\n模型 1. 模型概略 该文要做的事情是把自然语言序列\\(q = x_1 \\dots x_{|q|}\\)映射成逻辑表达式\\(a = y_1 \\dots y_{|a|}\\)。则条件概率\\(p(a|q)\\)可以分解为： \\(p(a|q)=\\prod_{t=1}^{|a|} p(y_t|y_{\u0026lt;t},q)\\)\n其中\\(y_{\u0026lt;t}=y_1 \\dots y_{t-1}\\)。\n2. seq2seq模型 seq2seq模型，深色（左边）的为encoder，浅色（右边）的为decoder。\n3. seq2tree模型 seq2tree的根本改变就是把decoder从一个单纯的序列RNN变成了考虑逻辑表达层次结构的复杂RNN模型。改进的地方就是在decoder的输出词表中加了一个“非叶子结点”\u0026lt;n\u0026gt;，用来表示该处还有子树。比如，上图中的预测逻辑表达式是“lambda $0 e (and (\u0026gt;(departure time $0) 1600:ti) (from $0 dallas:ci))”，每一层圆括号里包着的内容就是一棵子树。所以改进后的层次树decoder，通过先预测第一层，再在第一层的“非叶子结点”的基础上预测下一层，以此类推直到没有“非叶子结点”。整体的解码过程就像一个广度优先搜索一样进行着。\n4. attention 该文的attention计算方法和常见方法有点不一样，它使用decoder中当前hidden state和encoder中所有hidden state计算attention weight。 $$s_k^t = \\frac{ exp(\\textbf{h}^L_k \\cdot \\textbf{h}^L_t) }{ \\sum_{j=1}^{|q|}exp(\\textbf{h}^L_j \\cdot \\textbf{h}^L_t) }$$ 其中\\(\\textbf{h}^L_t\\)表示当前decoder的隐层状态，\\(\\textbf{h}^L_k\\)表示的是encoder的隐层状态。\n根据attention weight计算的context vector为 $$\\textbf{c}^t = \\sum_{k=1}^{|q|}s_k^t\\textbf{h}^L_k$$\ndecoder层的计算： $$\\textbf{h}^{att}_t = tanh(\\textbf{W}_1 \\textbf{h}^L_t + \\textbf{W}_2\\textbf{c}^t)$$ , \\(p(y_t|y_{\u0026lt;t},q)=softmax(\\textbf{W}_o\\textbf{h}^{att}_t)^T\\textbf{e}(y_t)\\) 其中\\(\\textbf{e}(y_t)\\)表示一个one-hot向量，用于获取\\(y_t\\)的概率。\n资源 其实验相关的代码： https://github.com/donglixp/lang2logic\n相关工作 semantic parsing的前人工作往往依赖于高质量的词典、人工构造的模板和领域特有的特征等。\n简评 seq2seq解决sentence to tree的问题已经不是新鲜事物了，比如 Vinyals做的句法分析的工作，但该文在seq-to-tree模型上的创新让人耳目一新。\n","date":1476403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476403200,"objectID":"3589a708104efc45a5c9cba6b89a7061","permalink":"https://sz128.github.io/post/paper-summary-seq2tree/","publishdate":"2016-10-14T00:00:00Z","relpermalink":"/post/paper-summary-seq2tree/","section":"post","summary":"Language to Logic Form with Neural Attention 作者 Li Dong, Mirella Lapata li.dong@ed.ac.uk, mlap@inf.ed.ac.uk\n单位 Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB","tags":[],"title":"Paper Summary Seq2tree","type":"post"},{"authors":["Su Zhu"],"categories":["paper_review"],"content":"论文推荐：zero-shot learning for SLU 引\n 在传统的口语语义理解中，我们往往都是在一个预先定义好的对话领域（predefined domain）内进行研究，比如领域专家或者开发人员在“航班”领域中定义语义槽（slot）“出发城市”、“出发时间”等，以及每个slot可能对应的值（“出发城市”——“北京，上海，\u0026hellip;”、“出发时间”——“上午八点，下午两点，\u0026hellip;”等）。\n即使假设我们可以负担的起对一个新领域的快速定义（定义用户意图、语义槽值等语义框架），如果我们计划使用有监督学习的方法训练一个语义解析器，那还需要足量的标注数据，比如在“航班”领域中有标注数据——“从[北京:from.city]去[上海:to.city]的机票”。但是，获取大量的标注数据是十分的耗时且人力成本很高的。所以zero-shot learning的原理被引入到SLU中，旨在利用少量甚至数量为零的训练数据得到一个覆盖所有领域内语义项的语义解析器。\nzero-shot learning的定义：学习分类器 \\(f : X \\rightarrow Y \\)，其中语义类别\\(Y\\)并没有在训练数据中出现过。即zero-shot learning的目标是为没有观察的标签学习相应的模型。\n关于zero-shot learning for SLU的推荐论文如下：\n Online Adaptative Zero-shot Learning Spoken Language Understanding Using Word-embedding. ICASSP 2015. Zero-shot Semantic Parser For Spoken Language Understanding. INTERSPEECH 2015. Adversarial Bandit For Online Interactive Active Learning Of Zero-shot Spoken Language Understanding. ICASSP 2016. A Model of Zero-Shot Learning of Spoken Language Understanding. EMNLP 2015. Zero-shot Learning Of Intent Embeddings For Expansion By Convolutional Deep Structured Semantic Models. ICASSP 2016.   Online Adaptative Zero-shot Learning Spoken Language Understanding Using Word-embedding \n1 作者 Emmanuel Ferreira, Bassam Jabaian and Fabrice Lefe`vre\n单位 CERI-LIA, University of Avignon, Avignon - France\n关键词 Spoken language understanding, word embedding, zero-shot learning, out-of-domain training data, online adaptation.\n来源 ICASSP 2015\n立题 不使用语义标注数据，仅使用ontology和外部词向量资源搭建SLU parser。\n模型 （1）zero-shot learning for SLU 如上图左侧所示，该模型由三部分组成：\n K, 语义知识库。是由ontology导出，每一列是一个acttype-slot-value的三元组，每一行则是该三元组的一个片段描述，比如“affirm()”的片段描述是“yes”、“yeah”等，“request(food)”的描述是“what foof is served”等。这些片段描述的生成可以由一些简单的规则自动生成。 F，语义特征空间，由外部词向量构成。可以计算不同“片段”之间的相似度。 上图左下方的parser。该parser遍历输入句子中的所有“片段”，放入语义特征空间F中和语义知识库K的每一行“片段描述”进行相似度计算，用knn寻找k个最接近的描述片段以及它们对应的acttype-slot-value的三元组。  （2）online adaptation 该文章除了上述不使用训练数据的zero-shot learning方法，考虑到zero-shot parser在实际中使用得到的反馈信息，还设计了一种在线自适应的方法。其反馈为一个0~1的分值 ，0表示negative，1表示positive。在线自适应算法会更新语义知识库K中的表值，如上图右侧所示。\n简评 该文章展示了zero-shot learning for SLU的初步工作，为构建一个零标注的语义解析器提供了不错的思路。\nZero-shot Semantic Parser For Spoken Language Understanding \n2 作者 Emmanuel Ferreira, Bassam Jabaian and Fabrice Lefe`vre\n单位 CERI-LIA, University of Avignon, Avignon - France\n关键词 Spoken language understanding, word embedding, zero-shot learning, out-of-domain training data, online adaptation.\n来源 INTERSPEECH 2015\n立题 不使用语义标注数据，仅使用ontology和外部词向量资源搭建SLU parser。\n模型 模型组成和上一篇文章是一样的，区别在于该文章引入CRF模型作为后处理模型，且该CRF模型是可扩展的（即可以添加标注数据对该CRF模型进行扩展训练）。\n简评 和上一篇文章的思路是一样的，唯一区别就是加入了CRF模型作为可扩展的后处理。\nAdversarial Bandit For Online Interactive Active Learning Of Zero-shot Spoken Language Understanding \n3 作者 Emmanuel Ferreira, Alexandre Reiffers Masson, Bassam Jabaian and Fabrice Lefe`vre\n单位 CERI-LIA, University of Avignon, France\n关键词 Spoken language understanding, zero-shot learning, bandit problem, out-of-domain training data, online adaptation.\n来源 ICASSP 2016\n立题 在上述Zero-Shot Semantic Parser(ZSSP)的基础上深入研究online adaptation方法，达到系统性能和用户付出之间的tradeoff。\n模型 引入Adversarial Bandit algorithm Exp3算法学习在线自适应策略（即有：请求用户简单0-1反馈、请求用户标注、跳过，这三个动作）。\n简评 文章给出了在线学习的新思路，至于其实用性，有待考证。\nA Model of Zero-Shot Learning of Spoken Language Understanding \n4 作者 Majid Yazdani，James Henderson majid.yazdani@unige.ch， james.henderson@xrce.xerox.com\n单位 Computer Science Department University of Geneva； Xerox Research Center Europe\n关键词 Spoken language understanding, zero-shot learning, domain expansion\n来源 EMNLP 2015\n立题 能否建立一个统计SLU模型，可以对训练数据中没有出现过的输入和输出有很好的泛化能力。\n模型 该文章的模型核心是将SLU任务的输入和所有可能输出都用词向量转成分布式的表达，然后用简单的余弦相似度计算输入和可能的输出直接的关联度。\n（1）输入的分布式表达 其过程是对句子中每一个位置的词计算得到一个向量表示，再对所有词的向量表示使用池化（pooling）的方法得出句子的分布式表达。计算某一个位置的词的向量表示的公式如下：\n$$ \\phi(U_i) = \\sigma(\\phi(w_i)W_{word}+\\phi(w_h)W_{parse_{R_k}}+\\phi(w_j)W_{previous}+\\phi(w_k)W_{next}) $$\n其中\\(\\phi(w)\\)是词\\(w\\)的词向量，\\(w_i\\)是当前词，\\(w_j\\)是前一个词，\\(w_k\\)是后一个词，\\(w_h\\)是句法依存树上当前词的父节点对应的词，而又根据两者的依存关系\\(R_k\\)的不同，设置的权值矩阵\\(W_{parse_{R_k}}\\)也不一样，故\\(W_{parse}\\)是一个三维tensor。\n（2）输出的分布式表达 针对所有可能的输出（比如DAtype-attribute-value三元组），作者也计算它们的分布式表达。计算过程同样利用了词向量，公式：\n$$ W_{a_j,att_k,val_l} = \\sigma([\\phi(a_j),\\phi(att_k),\\phi(val_l)]W_{ih})W_{ho} $$\n即将DAtype，attribute，value三者的词向量串联起来，过两层前馈神经网络。\n（3）训练 训练准则如下：\n$$ min_{\\theta}\\ \\frac{\\lambda}{2}{\\theta}^2 + \\sum_{U}max(0, 1-y\\sum_i \\phi(U_i)W_{a_j,att_k,val_l}^T)$$\n其中\\(\\theta\\)是模型的所有参数，y为1或者-1（根据输入U中是否包含相应的标签\\(\\{a_j,att_k,val_l\\}\\)）。\n该模型在领域扩展的任务上取得了不错的效果。\n资源 词向量：https://code.google.com/p/word2vec/\n实验数据：https://sites.google.com/site/parlanceprojectofficial/home/datarepository\n相关工作 在SLU中前人的工作主要是基于规则的方法，以及需要大量领域内数据的有监督学习的方法。\n简评 该方法对于领域扩展来说是非常实用的一种方法，但对于自适应到全新的领域上的效果还需要质疑。\nZero-shot Learning Of Intent Embeddings For Expansion By Convolutional Deep Structured Semantic Models \n5 作者 Yun-Nung Chen⋆† Dilek Hakkani-Tu ̈r† Xiaodong He†\nyvchen@cs.cmu.edu, dilek@ieee.org, xiaohe@microsoft.com\n单位 ⋆Carnegie Mellon University, Pittsburgh, PA, USA\n†Microsoft Research, Redmond, WA, USA\n关键词 zero-shot learning, spoken language understanding (SLU), spoken dialogue system (SDS), convolutional deep structured semantic model (CDSSM), embeddings, expansion.\n来源 ICASSP 2016\n立题 该文章专注于intent扩展，打破领域的界限，通过有限的数据，训练一个intent模型可以对数据中没有出现过的intent有很好的泛化能力。\n模型 该文章同样期望通过输入输出的分布式表达来获取泛化模型。举个例子来描述这种泛化过程，比如训练数据中出现了intent “find_movie”和“find_flight”，这样模型就可以学习到“find”作为intent一部分时的模式，进而对未在数据中出现过的“find_person”等intent的预测提供帮助。 该模型使用余弦距离计算输入和输出的相识度。在训练该模型时，作者提供了两种训练策略，一种基于鉴别式模型，一种基于生成式模型。\n（1）鉴别式模型 $$P(I|U) = \\frac{exp(CosSim(U,I))}{\\sum_{I\u0026rsquo;}exp(CosSim(U,I\u0026rsquo;))}$$\n（2）生成式模型 $$ P(U|I) = \\frac{exp(CosSim(U,I))}{\\sum_{U\u0026rsquo;}exp(CosSim(U\u0026rsquo;,I))} $$\nintent detection 针对上述两种模型，该文章给出了两类intent detection方法，一类是两种模型单独计算输入和可能的intent直接的余弦相似度，另一类是将两种模型计算得到的两个相似度加权求和。 $$ S_{Bi}(U,I) = \\lambda \\cdot S_P(U,I) + (1 − \\lambda) \\cdot S_G(U,I) $$\n相关工作 关于intent expansion，前人有利用知识图谱和搜索引擎click log来做跨领域intent识别的工作。\n简评 打破领域解析的intent识别听上去就很诱人。\n","date":1476403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476403200,"objectID":"d15d683aa559492542ea34719cb95903","permalink":"https://sz128.github.io/post/zero-shot-slu/","publishdate":"2016-10-14T00:00:00Z","relpermalink":"/post/zero-shot-slu/","section":"post","summary":"论文推荐：zero-shot learning for SLU 引\n 在传统的口语语义理解中，我们往往都是在一个预先定义好的对话领域（predefined domain）内进行研究，比如领域专家或者开发人员在“航班”领域中定义语义槽（slot）“出发城市”、“出发时间”等，以及每个slot可能对应的值（“出发城市”——“北京，上海，\u0026hellip;”、“出发时间”——“上午八点，下午两点，\u0026hellip;”等）。\n即使假设我们可以负担的起对一个新领域的快速定义（定义用户意图、语义槽值等语义框架），如果我们计划使用有监督学习的方法训练一个语义解析器，那还需要足量的标注数据，比如在“航班”领域中有标注数据——“从[北京:from.city]去[上海:to.city]的机票”。但是，获取大量的标注数据是十分的耗时且人力成本很高的。所以zero-shot learning的原理被引入到SLU中，旨在利用少量甚至数量为零的训练数据得到一个覆盖所有领域内语义项的语义解析器。\nzero-shot learning的定义：学习分类器 \\(f : X \\rightarrow Y \\)，其中语义类别\\(Y\\)并没有在训练数据中出现过。即zero-shot learning的目标是为没有观察的标签学习相应的模型。\n关于zero-shot learning for SLU的推荐论文如下：\n Online Adaptative Zero-shot Learning Spoken Language Understanding Using Word-embedding.","tags":[],"title":"论文推荐：zero-shot learning for spoken language understanding(SLU)","type":"post"},{"authors":null,"categories":[],"content":"","date":1473638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473638400,"objectID":"64a0a8d20d9276bf5d27ce8e9e9ff348","permalink":"https://sz128.github.io/publication/2016-is-dstc4/","publishdate":"2016-09-12T00:00:00Z","relpermalink":"/publication/2016-is-dstc4/","section":"publication","summary":"","tags":[],"title":"Hybrid Dialogue State Tracking for Real World Human-to-Human Dialogues","type":"publication"},{"authors":null,"categories":[],"content":"","date":1443398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443398400,"objectID":"efb9b85225651d3b45773046448ffd6e","permalink":"https://sz128.github.io/publication/2016-fcs-yu_kai/","publishdate":"2015-09-28T00:00:00Z","relpermalink":"/publication/2016-fcs-yu_kai/","section":"publication","summary":"","tags":[],"title":"Evolvable dialogue state tracking for statistical dialogue management","type":"publication"},{"authors":null,"categories":[],"content":"","date":1441152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441152000,"objectID":"81c25b187e047a824eab660ded1bf3a4","permalink":"https://sz128.github.io/publication/2015-sigdial-xie_qizhe/","publishdate":"2015-09-02T00:00:00Z","relpermalink":"/publication/2015-sigdial-xie_qizhe/","section":"publication","summary":"","tags":[],"title":"Recurrent Polynomial Network for Dialogue State Tracking with Mismatched Semantic Parsers","type":"publication"},{"authors":null,"categories":[],"content":"","date":1439337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439337600,"objectID":"db8b4d8fb2a95f63167d948ff6626c90","permalink":"https://sz128.github.io/publication/2015-taslp-yu_kai/","publishdate":"2015-08-12T00:00:00Z","relpermalink":"/publication/2015-taslp-yu_kai/","section":"publication","summary":"","tags":[],"title":"Constrained Markov Bayesian Polynomial for Efficient Dialogue State Tracking","type":"publication"},{"authors":null,"categories":[],"content":"","date":1434758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434758400,"objectID":"b062a4fd4769d6abd6b28ae4c873eadf","permalink":"https://sz128.github.io/publication/2014-sigdial-sun_kai/","publishdate":"2015-06-20T00:00:00Z","relpermalink":"/publication/2014-sigdial-sun_kai/","section":"publication","summary":"","tags":[],"title":"The SJTU System for Dialog State Tracking Challenge 2","type":"publication"},{"authors":null,"categories":[],"content":"","date":1418169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418169600,"objectID":"6cbbcb3e957ea896b0411b6e0f15d970","permalink":"https://sz128.github.io/publication/2014-slt-sun_kai/","publishdate":"2014-12-10T00:00:00Z","relpermalink":"/publication/2014-slt-sun_kai/","section":"publication","summary":"","tags":[],"title":"A Generalized Rule Based Tracker for Dialogue State Tracking","type":"publication"},{"authors":null,"categories":[],"content":"","date":1418169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418169600,"objectID":"11a27c3d5c40b1336a927e7a26d10f13","permalink":"https://sz128.github.io/publication/2014-slt-dstc3_slu/","publishdate":"2014-12-10T00:00:00Z","relpermalink":"/publication/2014-slt-dstc3_slu/","section":"publication","summary":"","tags":[],"title":"Semantic parser enhancement for dialogue domain extensions with little data","type":"publication"},{"authors":null,"categories":[],"content":"","date":1405036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405036800,"objectID":"b64bf98e54aac2f5168b072d52be6e27","permalink":"https://sz128.github.io/publication/2014-cjc-yu_kai/","publishdate":"2014-07-11T00:00:00Z","relpermalink":"/publication/2014-cjc-yu_kai/","section":"publication","summary":"","tags":[],"title":"Cognitive Technology in Task-oriented Dialogue Systems – Concepts, Advances and Future","type":"publication"}]