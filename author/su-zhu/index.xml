<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Su Zhu | Academic</title>
    <link>https://sz128.github.io/author/su-zhu/</link>
      <atom:link href="https://sz128.github.io/author/su-zhu/index.xml" rel="self" type="application/rss+xml" />
    <description>Su Zhu</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 10 Feb 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://sz128.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Su Zhu</title>
      <link>https://sz128.github.io/author/su-zhu/</link>
    </image>
    
    <item>
      <title>Paper Reading 2017 02 10</title>
      <link>https://sz128.github.io/post/paper-reading-2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://sz128.github.io/post/paper-reading-2017-02-10/</guid>
      <description>&lt;h1 id=&#34;2017鸡年第一波论文阅读总结及推荐&#34;&gt;2017鸡年第一波论文阅读总结及推荐&lt;/h1&gt;
&lt;section class=&#34;&#34; style=&#34;margin-top: 1.5em; margin-right: auto; margin-left: auto; padding-top: 0.5em; padding-bottom: 0.5em; border-style: solid none; border-top-width: 1px; border-top-color: rgb(63, 45, 42); font-weight: inherit; text-decoration: inherit; border-bottom-width: 1px; border-bottom-color: rgb(255, 255, 255);&#34;&gt;&lt;section style=&#34;margin-top: -1.8em; border-width: initial; border-style: none; border-color: initial; line-height: 1.4;&#34;&gt;&lt;p class=&#34;&#34; style=&#34;padding: 8px 23px; color: rgb(255, 255, 255); font-weight: inherit; text-align: center; text-decoration: inherit; box-shadow: rgb(104, 104, 202) 2px 2px 10px; display: inline-block; border-color: rgb(137, 18, 68) rgb(241, 60, 136) rgb(85, 18, 46) rgb(211, 36, 109); background-color: rgb(63, 45, 42);&#34;&gt;引&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;本次推荐论文如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Structured prediction models for RNN based sequence labeling in clinical text. EMNLP 2016.&lt;/li&gt;
&lt;li&gt;Sentence rewriting for semantic parsing. ACL 2016.&lt;/li&gt;
&lt;li&gt;Adversarial training methods for semi-supervised text classification. Under review as a conference paper at ICLR 2017.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Structured prediction models for RNN based sequence labeling in clinical text
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;1&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Abhyuday N Jagannatha, Hong Yu&lt;/p&gt;
&lt;h5 id=&#34;单位&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;University of Massachusetts, MA, USA
Bedford VAMC and CHOIR, MA, USA&lt;/p&gt;
&lt;h5 id=&#34;关键词&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;sequence labeling, skip chain CRF, Bi-LSTM&lt;/p&gt;
&lt;h5 id=&#34;来源&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;EMNLP 2016&lt;/p&gt;
&lt;h5 id=&#34;立题&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;解决医用领域的slot filling问题。&lt;/p&gt;
&lt;h5 id=&#34;模型&#34;&gt;模型&lt;/h5&gt;
&lt;h6 id=&#34;1bi-lstm-crf&#34;&gt;（1）Bi-LSTM CRF&lt;/h6&gt;
&lt;p&gt;Bi-LSTM 的输出层套上CRF的criterion，可以参考:[Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991.]&lt;/p&gt;
&lt;h6 id=&#34;2bi-lstm-crf-with-pairwise-modeling&#34;&gt;（2）Bi-LSTM CRF with pairwise modeling&lt;/h6&gt;
&lt;p&gt;因为很多label的连接在训练数据中出现的次数很少，无法被很好地学习到。于是作者把CRF中$$A_{y_t,y_{t+1}}$$的label transition score用一个简单的神经网络模型来预测，这个NN模型的输入是t时刻和t+1时刻的隐层状态。&lt;/p&gt;
&lt;h6 id=&#34;3bi-lstm-with-approximate-skip-chain-crf&#34;&gt;（3）Bi-LSTM with approximate Skip-chain CRF&lt;/h6&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2017-02-10-1.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;简评&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;这是一个对Bi-LSTM CRF深入研究的工作。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Sentence rewriting for semantic parsing
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;2&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-1&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Bo Chen, Le Sun, Xianpei Han, Bo An&lt;/p&gt;
&lt;h5 id=&#34;单位-1&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;State Key Laboratory of Computer Sciences
Institute of Software, Chinese Academy of Sciences, China.&lt;/p&gt;
&lt;h5 id=&#34;关键词-1&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Sentence rewriting, semantic parsing&lt;/p&gt;
&lt;h5 id=&#34;来源-1&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;ACL 2016&lt;/p&gt;
&lt;h5 id=&#34;立题-1&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;由于semantic parsing的目标logical form是依赖于ontology的词典的，所以有一些语义相同但是语言表达形式稀少的句子很难被解析正确。为了解决这个问题，该文章提出使用句子转写的方式，把语言表达形式稀少的句子转成意思相同且常见的句子。&lt;/p&gt;
&lt;h5 id=&#34;模型-1&#34;&gt;模型&lt;/h5&gt;
&lt;h6 id=&#34;1dictionary-based-rewriting&#34;&gt;（1）Dictionary-based Rewriting&lt;/h6&gt;
&lt;p&gt;基于字典的名词再解释。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2017-02-10-2.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;做法举例：“For instance, the word “daughter” is explained as “female child” in Wiktionary”&lt;/p&gt;
&lt;p&gt;使用到资源：Wiktionary&lt;/p&gt;
&lt;h6 id=&#34;2template-based-rewriting&#34;&gt;（2）Template-based Rewriting&lt;/h6&gt;
&lt;p&gt;基于句子模板的替换。
&lt;img src=&#34;https://sz128.github.io/img/posts/2017-02-10-3.png&#34; alt=&#34;Alt text&#34;&gt;
使用到资源：WikiAnswers&lt;/p&gt;
&lt;h5 id=&#34;简评-1&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;句子重写是一种解决数据稀疏问题的逆向方法。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Adversarial training methods for semi-supervised text classification
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;3&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-2&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Takeru Miyato, Andrew M Dai, Ian Goodfellow&lt;/p&gt;
&lt;h5 id=&#34;单位-2&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;Kyoto University, Google Brain and OpenAI&lt;/p&gt;
&lt;h5 id=&#34;关键词-2&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Adversarial training, virtual adversarial training, semi-supervised learning, text classification&lt;/p&gt;
&lt;h5 id=&#34;来源-2&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;ICLR 2017 （under review）&lt;/p&gt;
&lt;h5 id=&#34;立题-2&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;把GAN学习应用到NLP里来。&lt;/p&gt;
&lt;h5 id=&#34;模型-2&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2017-02-10-4.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;h6 id=&#34;1adversarial-training&#34;&gt;（1）Adversarial training&lt;/h6&gt;
&lt;p&gt;有监督学习。&lt;/p&gt;
&lt;h6 id=&#34;2virtual-adversarial-training&#34;&gt;（2）Virtual adversarial training&lt;/h6&gt;
&lt;p&gt;无监督学习。&lt;/p&gt;
&lt;h5 id=&#34;简评-2&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;提供了一种regularization的方案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Survey on Grammar Retrieval</title>
      <link>https://sz128.github.io/post/survey-on-grammar-retrieval/</link>
      <pubDate>Thu, 27 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://sz128.github.io/post/survey-on-grammar-retrieval/</guid>
      <description>&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#前言&#34;&gt;前言&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#论文调研&#34;&gt;论文调研&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#调研方向&#34;&gt;调研方向&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#调研方式&#34;&gt;调研方式&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#文章汇总&#34;&gt;文章汇总&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;在任务型的口语对话系统框架下，目前主流的口语语义理解方法主要分为基于规则、统计学习的方法。在建立一个口语语义解析器之初，领域专家或者开发者往往需要人为定义语义框架、语义槽以及符合对话场景的所有可能的槽值。基于规则的方法，则继续需要领域专家或者开发人员人为地编写、维护规则文法。基于统计学习的方法则需要数据标注人员标注大量的数据，用于有监督学习。所以现在主流的口语语义解析器的构建过程可以描述为如下几个步骤：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;定义指定领域的语义表示方式&lt;/li&gt;
&lt;li&gt;编写规则/(收集数据+人工标注)&lt;/li&gt;
&lt;li&gt;生成模型&lt;/li&gt;
&lt;li&gt;校验&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;然而其中有几大问题，使得任务型的口语义理解无法快速地构建或者领域迁移。&lt;/p&gt;
&lt;p&gt;第一个问题是“语义表示方式的定义”。目前绝大多数的方法都使用意图+槽值对的方式来表示一句话的语义。这样简单的设计对于单个领域或者应用是非常高效的，然而对于领域扩展、迁移却并不友好。比如天气领域中有语义槽“city”，机票领域中有语义槽“Fromcity”和“Tocity”，这三个语义槽在这种扁平化的设计里是属于同一层次的，然而这并不符合人类语言现象。是否改为“city”、“from.city”、“to.city”会好一点呢？那么有层次的语义槽设计具体应该是怎样的呢？&lt;/p&gt;
&lt;p&gt;第二个问题是“规则编写”和“数据标注”。数据收集和数据标注是非常耗费人力和财力的，而相比之下规则编写要轻松一些。然而目前人工编写规则有如下主要缺点：第一，人力消耗大；第二，难维护；第三，难移植到新领域。所以我们是否可以自动生成规则？是否可以提升规则系统的易维护性？是否可以实现规则的跨领域移植？&lt;/p&gt;
&lt;p&gt;那为什么我们选择基于规则检索的方法做口语义理解？原因之一是基于规则方法的可控性要高于数据驱动的基于统计学习的方法（加入一条新规则比加入一类新数据且重训模型要可控）；更重要的是语义规则是一种高层知识的体现，是很难直接从有限的数据中学习到。为此，我们希望从从基于规则的方法出发，力图解决上述几个问题，实现“开放式、可扩展”的口语语义理解。&lt;/p&gt;
&lt;h2 id=&#34;论文调研&#34;&gt;论文调研&lt;/h2&gt;
&lt;h3 id=&#34;调研方向&#34;&gt;调研方向&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;规则生成&lt;/li&gt;
&lt;li&gt;规则检索&lt;/li&gt;
&lt;li&gt;语义表示&lt;/li&gt;
&lt;li&gt;语义项自动导出&lt;/li&gt;
&lt;li&gt;规则与统计方法相结合&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;调研方式&#34;&gt;调研方式&lt;/h3&gt;
&lt;h4 id=&#34;已知的语义库&#34;&gt;已知的语义库&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;FrameNet/PropBank/AMR&lt;/li&gt;
&lt;li&gt;Freebase&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;google搜索&#34;&gt;google搜索&lt;/h4&gt;
&lt;p&gt;关键词&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grammar/rule/template retrieval [+semantic]&lt;/li&gt;
&lt;li&gt;semantic representation&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;会议期刊论文集&#34;&gt;会议期刊论文集&lt;/h4&gt;
&lt;p&gt;近三年相关论文，搜索关键词为:retrieval/rule/grammar/slot/semantic_representation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interspeech/ICASSP/ASRU/SLT/Sigdial&lt;/li&gt;
&lt;li&gt;ACL/EMNLP/COLING/NAACL&lt;/li&gt;
&lt;li&gt;TASLP&lt;/li&gt;
&lt;li&gt;AAAI/NIPS&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;文章汇总&#34;&gt;文章汇总&lt;/h3&gt;
&lt;h4 id=&#34;规则生成&#34;&gt;规则生成&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;SemEval-2014 Task 2: Grammar Induction for Spoken Dialogue Systems. SemEval 2014.&lt;/li&gt;
&lt;li&gt;SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using CF-IRF Rule Similarity. SemEval 2014.&lt;/li&gt;
&lt;li&gt;tucSage: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection. SemEval 2014.&lt;/li&gt;
&lt;li&gt;Web Data Harvesting for Speech Understanding Grammar Induction. Interspeech 2013.&lt;/li&gt;
&lt;li&gt;Using Lexical, Syntactic And Semantic Features For Non-terminal Grammar Rule Induction In Spoken Dialogue Systems. SLT 2014.&lt;/li&gt;
&lt;li&gt;Fusion Of Knowledge-based And Data-driven Approaches To Grammar Induction. Interspeech 2014.&lt;/li&gt;
&lt;li&gt;Semi-automatic Acquisition Of Domain-specific Semantic Structures. EuroSpeech 1999.&lt;/li&gt;
&lt;li&gt;Semiautomatic acquisition of semantic structures for understanding domain-specific natural language queries. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2002.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;规则检索&#34;&gt;规则检索&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Information Retrieval Techniques in Rule-based Expert Systems. CDAKO 1991.&lt;/li&gt;
&lt;li&gt;A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation. ACL 2016.&lt;/li&gt;
&lt;li&gt;Rule Selection with Soft Syntactic Features for String-to-Tree Statistical Machine Translation. EMNLP 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;语义表示&#34;&gt;语义表示&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Spoken language understanding. IEEE Signal Processing Magazine 2005.&lt;/li&gt;
&lt;li&gt;Spoken Language Understanding-Interpreting the signs given by a speech signal. IEEE Signal Processing Magazine 2008.&lt;/li&gt;
&lt;li&gt;A SURVEY ON ONTOLOGY CONSTRUCTION METHODOLOGIES. ECBS 2011.&lt;/li&gt;
&lt;li&gt;Semantic Representation.&lt;/li&gt;
&lt;li&gt;De-Conflated Semantic Representations.&lt;/li&gt;
&lt;li&gt;A Domain-independent Rule-based Framework for Event Extraction.&lt;/li&gt;
&lt;li&gt;Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity.&lt;/li&gt;
&lt;li&gt;Machine Comprehension using Rich Semantic Representations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;语义项自动导出&#34;&gt;语义项自动导出&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Unsupervised Induction And Filling Of Semantic Slots For Spoken Dialogue Systems Using Frame-semantic Parsing. ASRU 2013.&lt;/li&gt;
&lt;li&gt;Leveraging Frame Semantics And Distributional Semantics For Unsupervised Semantic Slot Induction In Spoken Dialogue Systems. SLT 2014.&lt;/li&gt;
&lt;li&gt;Dynamically Supporting Unexplored Domains In Conversational Interactions By Enriching Semantics With Neural Word Embeddings. SLT 2014.&lt;/li&gt;
&lt;li&gt;Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding. ACL 2015.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;统计规则&#34;&gt;统计+规则&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Harnessing Deep Neural Networks with Logic Rules. ACL 2016.&lt;/li&gt;
&lt;li&gt;Semantic Parsing using Distributional Semantics and Probabilistic Logic.  The ACL 2014 Workshop on Semantic Parsing.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary Seq2tree</title>
      <link>https://sz128.github.io/post/paper-summary-seq2tree/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://sz128.github.io/post/paper-summary-seq2tree/</guid>
      <description>&lt;h1 id=&#34;language-to-logic-form-with-neural-attention&#34;&gt;Language to Logic Form with Neural Attention&lt;/h1&gt;
&lt;h2 id=&#34;作者&#34;&gt;作者&lt;/h2&gt;
&lt;p&gt;Li Dong, Mirella Lapata &lt;br&gt;
&lt;a href=&#34;mailto:li.dong@ed.ac.uk&#34;&gt;li.dong@ed.ac.uk&lt;/a&gt;, &lt;a href=&#34;mailto:mlap@inf.ed.ac.uk&#34;&gt;mlap@inf.ed.ac.uk&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;单位&#34;&gt;单位&lt;/h2&gt;
&lt;p&gt;Institute for Language, Cognition and Computation &lt;br&gt;
School of Informatics, University of Edinburgh &lt;br&gt;
10 Crichton Street, Edinburgh EH8 9AB&lt;/p&gt;
&lt;h2 id=&#34;关键词&#34;&gt;关键词&lt;/h2&gt;
&lt;p&gt;sequence-to-sequence, sequence-to-tree, semantic parsing&lt;/p&gt;
&lt;h2 id=&#34;来源&#34;&gt;来源&lt;/h2&gt;
&lt;p&gt;ACL 2016&lt;/p&gt;
&lt;h2 id=&#34;立题&#34;&gt;立题&lt;/h2&gt;
&lt;p&gt;该文涉及的任务是semantic parsing，其目标是将一句话解析为正式的意图表示（比如一个逻辑表达式或者结构化的query）。作者首次将seq2seq引入该任务，并在普通seq2seq的decoder无法考虑逻辑表达式的层次结构的问题上，创造性地提出了层次树decoder。&lt;/p&gt;
&lt;h2 id=&#34;模型&#34;&gt;模型&lt;/h2&gt;
&lt;h3 id=&#34;1-模型概略&#34;&gt;1. 模型概略&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-09-26-pic1.png&#34; alt=&#34;Alt text&#34;&gt;
该文要做的事情是把自然语言序列\(q = x_1 \dots x_{|q|}\)映射成逻辑表达式\(a = y_1 \dots y_{|a|}\)。则条件概率\(p(a|q)\)可以分解为：
\(p(a|q)=\prod_{t=1}^{|a|} p(y_t|y_{&amp;lt;t},q)\)&lt;/p&gt;
&lt;p&gt;其中\(y_{&amp;lt;t}=y_1 \dots y_{t-1}\)。&lt;/p&gt;
&lt;h3 id=&#34;2-seq2seq模型&#34;&gt;2. seq2seq模型&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-09-26-pic2.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;seq2seq模型，深色（左边）的为encoder，浅色（右边）的为decoder。&lt;/p&gt;
&lt;h3 id=&#34;3-seq2tree模型&#34;&gt;3. seq2tree模型&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-09-26-pic3.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;seq2tree的根本改变就是把decoder从一个单纯的序列RNN变成了考虑逻辑表达层次结构的复杂RNN模型。改进的地方就是在decoder的输出词表中加了一个“非叶子结点”&amp;lt;n&amp;gt;，用来表示该处还有子树。比如，上图中的预测逻辑表达式是“lambda $0 e (and (&amp;gt;(departure
time $0) 1600:ti) (from $0 dallas:ci))”，每一层圆括号里包着的内容就是一棵子树。所以改进后的层次树decoder，通过先预测第一层，再在第一层的“非叶子结点”的基础上预测下一层，以此类推直到没有“非叶子结点”。整体的解码过程就像一个广度优先搜索一样进行着。&lt;/p&gt;
&lt;h3 id=&#34;4-attention&#34;&gt;4. attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-09-26-pic4.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;该文的attention计算方法和常见方法有点不一样，它使用decoder中当前hidden state和encoder中所有hidden state计算attention weight。
$$s_k^t = \frac{ exp(\textbf{h}^L_k \cdot \textbf{h}^L_t) }{ \sum_{j=1}^{|q|}exp(\textbf{h}^L_j \cdot \textbf{h}^L_t) }$$
其中\(\textbf{h}^L_t\)表示当前decoder的隐层状态，\(\textbf{h}^L_k\)表示的是encoder的隐层状态。&lt;/p&gt;
&lt;p&gt;根据attention weight计算的context vector为
$$\textbf{c}^t = \sum_{k=1}^{|q|}s_k^t\textbf{h}^L_k$$&lt;/p&gt;
&lt;p&gt;decoder层的计算：
$$\textbf{h}^{att}_t = tanh(\textbf{W}_1 \textbf{h}^L_t + \textbf{W}_2\textbf{c}^t)$$
, \(p(y_t|y_{&amp;lt;t},q)=softmax(\textbf{W}_o\textbf{h}^{att}_t)^T\textbf{e}(y_t)\)
其中\(\textbf{e}(y_t)\)表示一个one-hot向量，用于获取\(y_t\)的概率。&lt;/p&gt;
&lt;h2 id=&#34;资源&#34;&gt;资源&lt;/h2&gt;
&lt;p&gt;其实验相关的代码：
&lt;a href=&#34;https://github.com/donglixp/lang2logic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/donglixp/lang2logic&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;p&gt;semantic parsing的前人工作往往依赖于高质量的词典、人工构造的模板和领域特有的特征等。&lt;/p&gt;
&lt;h2 id=&#34;简评&#34;&gt;简评&lt;/h2&gt;
&lt;p&gt;seq2seq解决sentence to tree的问题已经不是新鲜事物了，比如
&lt;a href=&#34;https://arxiv.org/pdf/1412.7449.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vinyals&lt;/a&gt;做的句法分析的工作，但该文在seq-to-tree模型上的创新让人耳目一新。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>论文推荐：zero-shot learning for spoken language understanding(SLU)</title>
      <link>https://sz128.github.io/post/zero-shot-slu/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://sz128.github.io/post/zero-shot-slu/</guid>
      <description>&lt;h1 id=&#34;论文推荐zero-shot-learning-for-slu&#34;&gt;论文推荐：zero-shot learning for SLU&lt;/h1&gt;
&lt;section class=&#34;&#34; style=&#34;margin-top: 1.5em; margin-right: auto; margin-left: auto; padding-top: 0.5em; padding-bottom: 0.5em; border-style: solid none; border-top-width: 1px; border-top-color: rgb(63, 45, 42); font-weight: inherit; text-decoration: inherit; border-bottom-width: 1px; border-bottom-color: rgb(255, 255, 255);&#34;&gt;&lt;section style=&#34;margin-top: -1.8em; border-width: initial; border-style: none; border-color: initial; line-height: 1.4;&#34;&gt;&lt;p class=&#34;&#34; style=&#34;padding: 8px 23px; color: rgb(255, 255, 255); font-weight: inherit; text-align: center; text-decoration: inherit; box-shadow: rgb(104, 104, 202) 2px 2px 10px; display: inline-block; border-color: rgb(137, 18, 68) rgb(241, 60, 136) rgb(85, 18, 46) rgb(211, 36, 109); background-color: rgb(63, 45, 42);&#34;&gt;引&lt;/p&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;在传统的口语语义理解中，我们往往都是在一个预先定义好的对话领域（predefined domain）内进行研究，比如领域专家或者开发人员在“航班”领域中定义语义槽（slot）“出发城市”、“出发时间”等，以及每个slot可能对应的值（“出发城市”——“北京，上海，&amp;hellip;”、“出发时间”——“上午八点，下午两点，&amp;hellip;”等）。&lt;/p&gt;
&lt;p&gt;即使假设我们可以负担的起对一个新领域的快速定义（定义用户意图、语义槽值等语义框架），如果我们计划使用有监督学习的方法训练一个语义解析器，那还需要足量的标注数据，比如在“航班”领域中有标注数据——“从[北京:from.city]去[上海:to.city]的机票”。但是，获取大量的标注数据是十分的耗时且人力成本很高的。所以zero-shot learning的原理被引入到SLU中，旨在利用少量甚至数量为零的训练数据得到一个覆盖所有领域内语义项的语义解析器。&lt;/p&gt;
&lt;p&gt;zero-shot learning的定义：学习分类器 \(f : X \rightarrow Y \)，其中语义类别\(Y\)并没有在训练数据中出现过。即zero-shot learning的目标是为没有观察的标签学习相应的模型。&lt;/p&gt;
&lt;p&gt;关于zero-shot learning for SLU的推荐论文如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Online Adaptative Zero-shot Learning Spoken Language Understanding Using Word-embedding. ICASSP 2015.&lt;/li&gt;
&lt;li&gt;Zero-shot Semantic Parser For Spoken Language Understanding. INTERSPEECH 2015.&lt;/li&gt;
&lt;li&gt;Adversarial Bandit For Online Interactive Active Learning Of Zero-shot Spoken Language Understanding. ICASSP 2016.&lt;/li&gt;
&lt;li&gt;A Model of Zero-Shot Learning of Spoken Language Understanding. EMNLP 2015.&lt;/li&gt;
&lt;li&gt;Zero-shot Learning Of Intent Embeddings For Expansion By Convolutional Deep Structured Semantic Models. ICASSP 2016.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Online Adaptative Zero-shot Learning Spoken Language Understanding Using Word-embedding
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;1&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Emmanuel Ferreira, Bassam Jabaian and Fabrice Lefe`vre&lt;/p&gt;
&lt;h5 id=&#34;单位&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;CERI-LIA, University of Avignon, Avignon - France&lt;/p&gt;
&lt;h5 id=&#34;关键词&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Spoken language understanding, word embedding, zero-shot learning, out-of-domain training data, online adaptation.&lt;/p&gt;
&lt;h5 id=&#34;来源&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;ICASSP 2015&lt;/p&gt;
&lt;h5 id=&#34;立题&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;不使用语义标注数据，仅使用ontology和外部词向量资源搭建SLU parser。&lt;/p&gt;
&lt;h5 id=&#34;模型&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-10-10-1.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;h6 id=&#34;1zero-shot-learning-for-slu&#34;&gt;（1）zero-shot learning for SLU&lt;/h6&gt;
&lt;p&gt;如上图左侧所示，该模型由三部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;K, 语义知识库。是由ontology导出，每一列是一个acttype-slot-value的三元组，每一行则是该三元组的一个片段描述，比如“affirm()”的片段描述是“yes”、“yeah”等，“request(food)”的描述是“what foof is served”等。这些片段描述的生成可以由一些简单的规则自动生成。&lt;/li&gt;
&lt;li&gt;F，语义特征空间，由外部词向量构成。可以计算不同“片段”之间的相似度。&lt;/li&gt;
&lt;li&gt;上图左下方的parser。该parser遍历输入句子中的所有“片段”，放入语义特征空间F中和语义知识库K的每一行“片段描述”进行相似度计算，用knn寻找k个最接近的描述片段以及它们对应的acttype-slot-value的三元组。&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;2online-adaptation&#34;&gt;（2）online adaptation&lt;/h6&gt;
&lt;p&gt;该文章除了上述不使用训练数据的zero-shot learning方法，考虑到zero-shot parser在实际中使用得到的反馈信息，还设计了一种在线自适应的方法。其反馈为一个0~1的分值
，0表示negative，1表示positive。在线自适应算法会更新语义知识库K中的表值，如上图右侧所示。&lt;/p&gt;
&lt;h5 id=&#34;简评&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;该文章展示了zero-shot learning for SLU的初步工作，为构建一个零标注的语义解析器提供了不错的思路。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Zero-shot Semantic Parser For Spoken Language Understanding
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;2&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-1&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Emmanuel Ferreira, Bassam Jabaian and Fabrice Lefe`vre&lt;/p&gt;
&lt;h5 id=&#34;单位-1&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;CERI-LIA, University of Avignon, Avignon - France&lt;/p&gt;
&lt;h5 id=&#34;关键词-1&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Spoken language understanding, word embedding, zero-shot learning, out-of-domain training data, online adaptation.&lt;/p&gt;
&lt;h5 id=&#34;来源-1&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;INTERSPEECH 2015&lt;/p&gt;
&lt;h5 id=&#34;立题-1&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;不使用语义标注数据，仅使用ontology和外部词向量资源搭建SLU parser。&lt;/p&gt;
&lt;h5 id=&#34;模型-1&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-10-10-2.png&#34; alt=&#34;Alt text&#34;&gt;
模型组成和上一篇文章是一样的，区别在于该文章引入CRF模型作为后处理模型，且该CRF模型是可扩展的（即可以添加标注数据对该CRF模型进行扩展训练）。&lt;/p&gt;
&lt;h5 id=&#34;简评-1&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;和上一篇文章的思路是一样的，唯一区别就是加入了CRF模型作为可扩展的后处理。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Adversarial Bandit For Online Interactive Active Learning Of Zero-shot Spoken Language Understanding
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;3&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-2&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Emmanuel Ferreira, Alexandre Reiffers Masson, Bassam Jabaian and Fabrice Lefe`vre&lt;/p&gt;
&lt;h5 id=&#34;单位-2&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;CERI-LIA, University of Avignon, France&lt;/p&gt;
&lt;h5 id=&#34;关键词-2&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Spoken language understanding, zero-shot learning, bandit problem, out-of-domain training data, online adaptation.&lt;/p&gt;
&lt;h5 id=&#34;来源-2&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;ICASSP 2016&lt;/p&gt;
&lt;h5 id=&#34;立题-2&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;在上述Zero-Shot Semantic Parser(ZSSP)的基础上深入研究online adaptation方法，达到系统性能和用户付出之间的tradeoff。&lt;/p&gt;
&lt;h5 id=&#34;模型-2&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;引入Adversarial Bandit algorithm Exp3算法学习在线自适应策略（即有：请求用户简单0-1反馈、请求用户标注、跳过，这三个动作）。&lt;/p&gt;
&lt;h5 id=&#34;简评-2&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;文章给出了在线学习的新思路，至于其实用性，有待考证。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
A Model of Zero-Shot Learning of Spoken Language Understanding
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;4&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-3&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Majid Yazdani，James Henderson
&lt;a href=&#34;mailto:majid.yazdani@unige.ch&#34;&gt;majid.yazdani@unige.ch&lt;/a&gt;， &lt;a href=&#34;mailto:james.henderson@xrce.xerox.com&#34;&gt;james.henderson@xrce.xerox.com&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;单位-3&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;Computer Science Department University of Geneva； Xerox Research Center Europe&lt;/p&gt;
&lt;h5 id=&#34;关键词-3&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;Spoken language understanding, zero-shot learning, domain expansion&lt;/p&gt;
&lt;h5 id=&#34;来源-3&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;EMNLP 2015&lt;/p&gt;
&lt;h5 id=&#34;立题-3&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;能否建立一个统计SLU模型，可以对训练数据中没有出现过的输入和输出有很好的泛化能力。&lt;/p&gt;
&lt;h5 id=&#34;模型-3&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://sz128.github.io/img/posts/2016-10-10-3.png&#34; alt=&#34;Alt text&#34;&gt;
该文章的模型核心是将SLU任务的输入和所有可能输出都用词向量转成分布式的表达，然后用简单的余弦相似度计算输入和可能的输出直接的关联度。&lt;/p&gt;
&lt;h6 id=&#34;1输入的分布式表达&#34;&gt;（1）输入的分布式表达&lt;/h6&gt;
&lt;p&gt;其过程是对句子中每一个位置的词计算得到一个向量表示，再对所有词的向量表示使用池化（pooling）的方法得出句子的分布式表达。计算某一个位置的词的向量表示的公式如下：&lt;/p&gt;
&lt;p&gt;$$ \phi(U_i) = \sigma(\phi(w_i)W_{word}+\phi(w_h)W_{parse_{R_k}}+\phi(w_j)W_{previous}+\phi(w_k)W_{next}) $$&lt;/p&gt;
&lt;p&gt;其中\(\phi(w)\)是词\(w\)的词向量，\(w_i\)是当前词，\(w_j\)是前一个词，\(w_k\)是后一个词，\(w_h\)是句法依存树上当前词的父节点对应的词，而又根据两者的依存关系\(R_k\)的不同，设置的权值矩阵\(W_{parse_{R_k}}\)也不一样，故\(W_{parse}\)是一个三维tensor。&lt;/p&gt;
&lt;h6 id=&#34;2输出的分布式表达&#34;&gt;（2）输出的分布式表达&lt;/h6&gt;
&lt;p&gt;针对所有可能的输出（比如DAtype-attribute-value三元组），作者也计算它们的分布式表达。计算过程同样利用了词向量，公式：&lt;/p&gt;
&lt;p&gt;$$ W_{a_j,att_k,val_l} = \sigma([\phi(a_j),\phi(att_k),\phi(val_l)]W_{ih})W_{ho} $$&lt;/p&gt;
&lt;p&gt;即将DAtype，attribute，value三者的词向量串联起来，过两层前馈神经网络。&lt;/p&gt;
&lt;h6 id=&#34;3训练&#34;&gt;（3）训练&lt;/h6&gt;
&lt;p&gt;训练准则如下：&lt;br&gt;
$$ min_{\theta}\ \frac{\lambda}{2}{\theta}^2 + \sum_{U}max(0, 1-y\sum_i \phi(U_i)W_{a_j,att_k,val_l}^T)$$&lt;br&gt;
其中\(\theta\)是模型的所有参数，y为1或者-1（根据输入U中是否包含相应的标签\(\{a_j,att_k,val_l\}\)）。&lt;/p&gt;
&lt;p&gt;该模型在领域扩展的任务上取得了不错的效果。&lt;/p&gt;
&lt;h5 id=&#34;资源&#34;&gt;资源&lt;/h5&gt;
&lt;p&gt;词向量：https://code.google.com/p/word2vec/&lt;br&gt;
实验数据：https://sites.google.com/site/parlanceprojectofficial/home/datarepository&lt;/p&gt;
&lt;h5 id=&#34;相关工作&#34;&gt;相关工作&lt;/h5&gt;
&lt;p&gt;在SLU中前人的工作主要是基于规则的方法，以及需要大量领域内数据的有监督学习的方法。&lt;/p&gt;
&lt;h5 id=&#34;简评-3&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;该方法对于领域扩展来说是非常实用的一种方法，但对于自适应到全新的领域上的效果还需要质疑。&lt;/p&gt;
&lt;section style=&#34;display: inline-block;&#34;&gt;&lt;p style=&#34;margin-top: 5px; padding: 5px 10px; color: rgb(63, 45, 42); border-bottom: 2px solid rgb(63, 45, 42); text-align: center; overflow: hidden;&#34;&gt;&lt;span style=&#34;color: inherit;&#34;&gt;
Zero-shot Learning Of Intent Embeddings For Expansion By Convolutional Deep Structured Semantic Models
&lt;/span&gt;&lt;/p&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: 4px; font-family: inherit; font-weight: bolder; border-bottom: 1px dashed rgb(63, 45, 42); border-top-color: rgb(63, 45, 42); border-right-color: rgb(63, 45, 42); border-left-color: rgb(63, 45, 42); overflow: hidden;&#34;&gt;&lt;/section&gt;&lt;section class=&#34;&#34; style=&#34;margin-top: -30px; margin-bottom: 0.5em; margin-left: 30px; width: 25px; color: rgb(63, 45, 42); overflow: hidden; height: 28px; line-height: 26px; border-width: 2.6px; border-style: solid; border-color: rgb(63, 45, 42); border-radius: 20%; float: left; text-align: center; background-color: rgb(244, 244, 244);&#34;&gt;&lt;span class=&#34;&#34; style=&#34;color: inherit;&#34;&gt;5&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;
&lt;h5 id=&#34;作者-4&#34;&gt;作者&lt;/h5&gt;
&lt;p&gt;Yun-Nung Chen⋆† Dilek Hakkani-Tu ̈r† Xiaodong He†&lt;br&gt;
&lt;a href=&#34;mailto:yvchen@cs.cmu.edu&#34;&gt;yvchen@cs.cmu.edu&lt;/a&gt;, &lt;a href=&#34;mailto:dilek@ieee.org&#34;&gt;dilek@ieee.org&lt;/a&gt;, &lt;a href=&#34;mailto:xiaohe@microsoft.com&#34;&gt;xiaohe@microsoft.com&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;单位-4&#34;&gt;单位&lt;/h5&gt;
&lt;p&gt;⋆Carnegie Mellon University, Pittsburgh, PA, USA&lt;br&gt;
†Microsoft Research, Redmond, WA, USA&lt;/p&gt;
&lt;h5 id=&#34;关键词-4&#34;&gt;关键词&lt;/h5&gt;
&lt;p&gt;zero-shot learning, spoken language understanding (SLU), spoken dialogue system (SDS), convolutional deep structured semantic model (CDSSM), embeddings, expansion.&lt;/p&gt;
&lt;h5 id=&#34;来源-4&#34;&gt;来源&lt;/h5&gt;
&lt;p&gt;ICASSP 2016&lt;/p&gt;
&lt;h5 id=&#34;立题-4&#34;&gt;立题&lt;/h5&gt;
&lt;p&gt;该文章专注于intent扩展，打破领域的界限，通过有限的数据，训练一个intent模型可以对数据中没有出现过的intent有很好的泛化能力。&lt;/p&gt;
&lt;h5 id=&#34;模型-4&#34;&gt;模型&lt;/h5&gt;
&lt;p&gt;该文章同样期望通过输入输出的分布式表达来获取泛化模型。举个例子来描述这种泛化过程，比如训练数据中出现了intent “find_movie”和“find_flight”，这样模型就可以学习到“find”作为intent一部分时的模式，进而对未在数据中出现过的“find_person”等intent的预测提供帮助。
&lt;img src=&#34;https://sz128.github.io/img/posts/2016-10-10-4.png&#34; alt=&#34;Alt text&#34;&gt;
该模型使用余弦距离计算输入和输出的相识度。在训练该模型时，作者提供了两种训练策略，一种基于鉴别式模型，一种基于生成式模型。&lt;/p&gt;
&lt;h6 id=&#34;1鉴别式模型&#34;&gt;（1）鉴别式模型&lt;/h6&gt;
&lt;p&gt;$$P(I|U) = \frac{exp(CosSim(U,I))}{\sum_{I&amp;rsquo;}exp(CosSim(U,I&amp;rsquo;))}$$&lt;/p&gt;
&lt;h6 id=&#34;2生成式模型&#34;&gt;（2）生成式模型&lt;/h6&gt;
&lt;p&gt;$$ P(U|I) = \frac{exp(CosSim(U,I))}{\sum_{U&amp;rsquo;}exp(CosSim(U&amp;rsquo;,I))} $$&lt;/p&gt;
&lt;h6 id=&#34;intent-detection&#34;&gt;intent detection&lt;/h6&gt;
&lt;p&gt;针对上述两种模型，该文章给出了两类intent detection方法，一类是两种模型单独计算输入和可能的intent直接的余弦相似度，另一类是将两种模型计算得到的两个相似度加权求和。
$$ S_{Bi}(U,I) = \lambda \cdot S_P(U,I) + (1 − \lambda) \cdot S_G(U,I) $$&lt;/p&gt;
&lt;h5 id=&#34;相关工作-1&#34;&gt;相关工作&lt;/h5&gt;
&lt;p&gt;关于intent expansion，前人有利用知识图谱和搜索引擎click log来做跨领域intent识别的工作。&lt;/p&gt;
&lt;h5 id=&#34;简评-4&#34;&gt;简评&lt;/h5&gt;
&lt;p&gt;打破领域解析的intent识别听上去就很诱人。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
